{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "authors.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcing of authors.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully scraped and saved to authors.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_main_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    quotes_data = []\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "    \n",
    "    for quote in quotes:\n",
    "        author_name = quote.find('small', class_='author').text if quote.find('small', class_='author') else None\n",
    "        author_about_link = quote.find('a')['href'] if quote.find('a') else None\n",
    "        if author_about_link:\n",
    "            about_url = url + author_about_link\n",
    "            author_info = scrape_author_details(about_url)\n",
    "            quotes_data.append(author_info)\n",
    "    \n",
    "    return quotes_data\n",
    "\n",
    "def scrape_author_details(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    fullname = soup.find('h3', class_='author-title').text.strip() if soup.find('h3', class_='author-title') else 'N/A'\n",
    "    born_date = soup.find('span', class_='author-born-date').text.strip() if soup.find('span', class_='author-born-date') else 'N/A'\n",
    "    born_location = soup.find('span', class_='author-born-location').text.strip() if soup.find('span', class_='author-born-location') else 'N/A'\n",
    "    description = soup.find('div', class_='author-description').text.strip() if soup.find('div', class_='author-description') else 'N/A'\n",
    "    \n",
    "    return {\n",
    "        'fullname': fullname,\n",
    "        'born_date': born_date,\n",
    "        'born_location': born_location,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "\n",
    "base_url = 'https://quotes.toscrape.com'\n",
    "\n",
    "\n",
    "authors_data = scrape_main_page(base_url)\n",
    "\n",
    "\n",
    "with open('authors.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(authors_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Data has been successfully scraped and saved to authors.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading authors to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted: 10\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import json\n",
    "\n",
    "client = MongoClient(\n",
    "    \"mongodb+srv://vkbyba:NuXMwhyumcClinLQ@bybadb.ydf2ryu.mongodb.net/\",\n",
    "    server_api=ServerApi('1')\n",
    ")\n",
    "\n",
    "db = client.book\n",
    "collection = db.authors\n",
    "\n",
    "\n",
    "with open('authors.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "if isinstance(data, list):\n",
    "    insert_result = collection.insert_many(data)\n",
    "else:\n",
    "    insert_result = collection.insert_one(data)\n",
    "\n",
    "print(f\"Documents inserted: {len(insert_result.inserted_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qoutes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcing of quotas.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def fetch_quotes(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "\n",
    "    quote_blocks = soup.find_all('div', class_='quote')\n",
    "    \n",
    "    quotes_list = []\n",
    "    \n",
    "\n",
    "    for quote_block in quote_blocks:\n",
    "        quote_text = quote_block.find('span', class_='text').text\n",
    "        \n",
    "\n",
    "        author = quote_block.find('small', class_='author').text\n",
    "        \n",
    "\n",
    "        tags = [tag.text for tag in quote_block.find_all('a', class_='tag')]\n",
    "        \n",
    "\n",
    "        quotes_list.append({\n",
    "            'tags': tags,\n",
    "            'author': author,\n",
    "            'quote': quote_text\n",
    "        })\n",
    "    \n",
    "    return quotes_list\n",
    "\n",
    "def save_quotes_to_json(quotes, filename):\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(quotes, file, indent=4)\n",
    "\n",
    "\n",
    "url = 'https://quotes.toscrape.com/page/1/'\n",
    "\n",
    "\n",
    "quotes_data = fetch_quotes(url)\n",
    "\n",
    "\n",
    "json_filename = 'C:/Users/Admin/Documents/GitHub/goit-ds-hw-03/quotes.json' # Adjust the path if needed\n",
    "\n",
    "\n",
    "save_quotes_to_json(quotes_data, json_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload of quotas to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents inserted: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import json\n",
    "\n",
    "client = MongoClient(\n",
    "    \"mongodb+srv://vkbyba:NuXMwhyumcClinLQ@bybadb.ydf2ryu.mongodb.net/\",\n",
    "    server_api=ServerApi('1')\n",
    ")\n",
    "\n",
    "db = client.book\n",
    "collection = db.quotes\n",
    "\n",
    "\n",
    "with open('quotes.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "if isinstance(data, list):\n",
    "    insert_result = collection.insert_many(data)\n",
    "else:\n",
    "    insert_result = collection.insert_one(data)\n",
    "\n",
    "print(f\"Documents inserted: {len(insert_result.inserted_ids)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
